{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":256618,"sourceType":"datasetVersion","datasetId":107620},{"sourceId":639622,"sourceType":"datasetVersion","datasetId":316368},{"sourceId":653195,"sourceType":"datasetVersion","datasetId":325566},{"sourceId":671851,"sourceType":"datasetVersion","datasetId":338555},{"sourceId":1877714,"sourceType":"datasetVersion","datasetId":1118008},{"sourceId":9695632,"sourceType":"datasetVersion","datasetId":5928199},{"sourceId":9862609,"sourceType":"datasetVersion","datasetId":6034593}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Pytorch Audio Emotion Classifier - Neel Patel","metadata":{}},{"cell_type":"markdown","source":"# 1. Data Preprocessing","metadata":{}},{"cell_type":"markdown","source":"## Go to !End of DataPreprocessing to run the cells before this","metadata":{}},{"cell_type":"code","source":"# Import packages!\n\nimport os\nimport shutil\nfrom zipfile import ZipFile\nimport time\nimport random\nfrom tqdm import tqdm\n\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport scipy\nimport librosa\n\nfrom IPython.display import Audio\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, Subset\nimport torchaudio\nfrom torch.nn.utils.rnn import pad_sequence\nimport torch.nn.functional as F\nimport torchaudio.transforms as T\nfrom sklearn.model_selection import train_test_split\n\nimport torch.multiprocessing as mp\n\nmp.set_start_method(\"spawn\", force=True)\n\n#Use GPU acceleration if possible\nif torch.cuda.is_available():\n    device = \"cuda\"\nelse:\n    device = \"cpu\"\n   \nprint(f'Using {device}') \n\n# Set seeds for reproducibility\n\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)  # If using CUDA\n\nAUGMENTATIONS_LOADED = True","metadata":{"execution":{"iopub.status.busy":"2024-11-12T22:10:23.327326Z","iopub.execute_input":"2024-11-12T22:10:23.327640Z","iopub.status.idle":"2024-11-12T22:10:30.814011Z","shell.execute_reply.started":"2024-11-12T22:10:23.327605Z","shell.execute_reply":"2024-11-12T22:10:30.813004Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Using cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"#Import file_path\n\n#Both sex\nRAVDESS_path ='/kaggle/input/speech-emotion-recognition-en/Ravdess/audio_speech_actors_01-24/'\n\n#Both Sex\nCrema_path = '/kaggle/input/speech-emotion-recognition-en/Crema/'\ncrema_metadata_df = pd.read_csv('/kaggle/input/crema-metadata-extra-information/VideoDemographics.csv')\n\n#Only male\nSAVEE_path = '/kaggle/input/speech-emotion-recognition-en/Savee/' \n\n#Only female\nTESS_path = '/kaggle/input/speech-emotion-recognition-en/Tess/'\n\n\nif os.path.exists('/kaggle/input/metadata-and-augmentations'):\n\n    augmented_dir = '/kaggle/input/metadata-and-augmentations'\n    \n    augmented_training_df = pd.read_csv(augmented_dir+'/augmented_training_df.csv')\n    testing_df = pd.read_csv(augmented_dir+'/testing_df.csv')\n\n    AUGMENTATIONS_LOADED = True\n    os.chdir(augmented_dir)\n    print('Successfully loaded previous augmentations')\n\nelse:\n    AUGMENTATIONS_LOADED = False\n    print('Go to !End of DataPreprocessing cell in table of contents to run the cells before this in order to get dataaugmentations')\n\n#AUGMENTATIONS_LOADED = False\n#os.chdir('/kaggle/working/')","metadata":{"execution":{"iopub.status.busy":"2024-11-12T22:10:30.815872Z","iopub.execute_input":"2024-11-12T22:10:30.816747Z","iopub.status.idle":"2024-11-12T22:10:30.950682Z","shell.execute_reply.started":"2024-11-12T22:10:30.816700Z","shell.execute_reply":"2024-11-12T22:10:30.949771Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Successfully loaded previous augmentations\n","output_type":"stream"}]},{"cell_type":"code","source":"#crema_metadata_df.loc[crema_metadata_df['Ethnicity'] == 'Hispanic']\ncrema_metadata_df.loc[crema_metadata_df['ActorID'] == 1013]","metadata":{"execution":{"iopub.status.busy":"2024-11-12T22:10:30.951785Z","iopub.execute_input":"2024-11-12T22:10:30.952137Z","iopub.status.idle":"2024-11-12T22:10:30.974290Z","shell.execute_reply.started":"2024-11-12T22:10:30.952101Z","shell.execute_reply":"2024-11-12T22:10:30.973252Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"    ActorID  Age     Sex       Race Ethnicity\n12     1013   22  Female  Caucasian  Hispanic","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ActorID</th>\n      <th>Age</th>\n      <th>Sex</th>\n      <th>Race</th>\n      <th>Ethnicity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>12</th>\n      <td>1013</td>\n      <td>22</td>\n      <td>Female</td>\n      <td>Caucasian</td>\n      <td>Hispanic</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def RAVDESS_extractor(audio_dir):\n    data_list = []\n    columns = ['Filename','Filepath','Gender','Emotion','Emotional Intensity']\n    RAV_metadata_df = pd.DataFrame(columns = columns)\n    \n    # Map identifiers to their corresponding values\n    emotion_dict = {\n      \"01\": \"neutral\", \"02\": \"neutral\", \"03\": \"happy\", \"04\": \"sad\",\n      \"05\": \"angry\", \"06\": \"fear\", \"07\": \"disgust\", \"08\": \"surprised\"\n    }\n    \n    intensity_dict = {\"01\": \"medium\", \"02\": \"high\"}\n    statement_dict = {\"01\": \"Kids are talking by the door\", \"02\": \"Dogs are sitting by the door\"}\n    \n    \n    data_list = []\n    for actor_folder in os.listdir(audio_dir):\n      actor_path = os.path.join(audio_dir, actor_folder)\n    \n      if os.path.isdir(actor_path):  # Check if it's a folder\n            for file in os.listdir(actor_path):\n                if file.endswith(\".wav\"):\n                    parts = file.split(\".\")[0].split(\"-\") #first split the .wav extension then the '-'\n    \n                    # Extract metadata from the filename\n                    modality = parts[0]  # Not used, as itâ€™s audio-only for now\n                    vocal_channel = \"speech\" if parts[1] == \"01\" else \"song\"\n                    emotion = emotion_dict[parts[2]]\n                    emotional_intensity = intensity_dict[parts[3]]\n                    statement = statement_dict[parts[4]]\n                    actor_id = int(parts[6])\n                    gender = \"male\" if actor_id % 2 != 0 else \"female\"\n                    file_path = os.path.join(actor_path, file)  # Full path to the file\n                    \n                    # Append to datalist (ignoring the repetition)\n                    data_list.append({\n                        'Filename': file,\n                        'Filepath':file_path,\n                        'Gender': gender,\n                        'Emotion': emotion,\n                        'Emotional Intensity': emotional_intensity\n                    })\n    \n    df_addon = pd.DataFrame(data_list)\n    RAV_metadata_df = pd.concat([RAV_metadata_df, df_addon], ignore_index=True)\n\n    return RAV_metadata_df\n\ndef CREMA_extractor(audio_dir,crema_metadata_df):\n    data_list = []\n    emotion_map_dict = {'SAD':'sad',\n                       'ANG':'angry',\n                       'DIS':'disgust',\n                       'FEA':'fear',\n                       'HAP':'happy',\n                       'NEU':'neutral'}\n    intensity_dict = {'LO':'low',\n                     'MD':'medium',\n                     'HI':'high',\n                     'XX':'unknown',\n                     'X':'unknown'}\n\n    columns = ['Filename','Filepath','Gender','Emotion','Emotional Intensity']\n    crema_organized_df = pd.DataFrame(columns = columns)\n    \n    for file in os.listdir(audio_dir):\n        parts = file.split('.')[0].split('_')\n\n        file_name = file\n        file_path = os.path.join(audio_dir,file)\n        actor_id = int(parts[0])\n\n        gender = crema_metadata_df.loc[crema_metadata_df['ActorID'] == actor_id]['Sex'].values[0].lower()\n        emotion = emotion_map_dict[parts[2]]\n\n        #debugging\n        #print(file_name)\n        intensity = intensity_dict[parts[3]]\n\n        data_list.append({'Filename': file_name,\n                         'Filepath':file_path,\n                         'Gender':gender,\n                         'Emotion':emotion,\n                         'Emotional Intensity':intensity})\n\n    df_addon = pd.DataFrame(data_list)\n    crema_organized_df = pd.concat([crema_organized_df,df_addon],ignore_index=True)\n    return crema_organized_df\n\ndef SAVEE_extractor(audio_dir):\n    columns = ['Filename','Filepath','Gender','Emotion','Emotional Intensity']\n    savee_metadata_df = pd.DataFrame(columns = columns)\n\n    data_list = []\n    \n    emotion_map_dict = {'sa':'sad',\n                       'a':'angry',\n                       'd':'disgust',\n                       'f':'fear',\n                       'h':'happy',\n                       'n':'neutral',\n                        'su':'surprised'}\n\n    for file in os.listdir(audio_dir):\n        parts = file.split('.')[0].split('_')\n\n        file_name = file\n        file_path = os.path.join(audio_dir,file)\n        gender = 'male'\n        \n        emotion_code = \"\".join([s for s in parts[1] if s.isalpha()])\n        emotion = emotion_map_dict[emotion_code]\n        intensity = 'unknown'\n\n        data_list.append({'Filename': file_name,\n                         'Filepath':file_path,\n                         'Gender':gender,\n                         'Emotion':emotion,\n                         'Emotional Intensity':intensity})\n\n    df_addon = pd.DataFrame(data_list)\n    savee_metadata_df = pd.concat([savee_metadata_df,df_addon],ignore_index=True)\n    return savee_metadata_df\n\ndef TESS_extractor(audio_dir):\n    columns = ['Filename','Filepath','Gender','Emotion','Emotional Intensity']\n    tess_metadata_df = pd.DataFrame(columns = columns)\n    \n    emotion_map_dict = {'sad':'sad',\n                       'angry':'angry',\n                       'disgust':'disgust',\n                       'fear':'fear',\n                       'happy':'happy',\n                       'neutral':'neutral',\n                       'ps':'surprised'}\n    data_list = []\n    \n    for folder in os.listdir(audio_dir):\n      folder_path = os.path.join(audio_dir, folder)\n    \n      if os.path.isdir(folder_path):  # Check if it's a folder\n            for file in os.listdir(folder_path):\n                if file.endswith(\".wav\"):\n                    file_name = file\n                    file_path = os.path.join(folder_path, file)\n                    \n                    parts = file.split('.')[0].split('_')\n                    emotion = emotion_map_dict[parts[2].lower()]\n                    intensity = 'unknown'\n                    gender = 'female'\n                    \n                    data_list.append({'Filename': file_name,\n                         'Filepath':file_path,\n                         'Gender':gender,\n                         'Emotion':emotion,\n                         'Emotional Intensity':intensity})\n\n    df_addon = pd.DataFrame(data_list)\n    tess_metadata_df = pd.concat([tess_metadata_df,df_addon],ignore_index=True)\n    return tess_metadata_df","metadata":{"execution":{"iopub.status.busy":"2024-11-12T22:11:19.375190Z","iopub.execute_input":"2024-11-12T22:11:19.375574Z","iopub.status.idle":"2024-11-12T22:11:19.403235Z","shell.execute_reply.started":"2024-11-12T22:11:19.375540Z","shell.execute_reply":"2024-11-12T22:11:19.402199Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Get all the datasets metadata dataframe\nravdess_metadata_df = RAVDESS_extractor(RAVDESS_path)\ncrema_organized_df = CREMA_extractor(Crema_path,crema_metadata_df)\nsavee_metadata_df = SAVEE_extractor(SAVEE_path)\ntess_metadata_df = TESS_extractor(TESS_path)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-12T22:11:20.085918Z","iopub.execute_input":"2024-11-12T22:11:20.086318Z","iopub.status.idle":"2024-11-12T22:11:25.401591Z","shell.execute_reply.started":"2024-11-12T22:11:20.086280Z","shell.execute_reply":"2024-11-12T22:11:25.400765Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"ravdess_metadata_df['Emotion'].unique()\ncrema_organized_df['Emotion'].unique()\nsavee_metadata_df['Emotion'].unique()\ntess_metadata_df['Emotion'].unique()","metadata":{"execution":{"iopub.status.busy":"2024-11-12T22:11:25.403055Z","iopub.execute_input":"2024-11-12T22:11:25.403364Z","iopub.status.idle":"2024-11-12T22:11:25.416287Z","shell.execute_reply.started":"2024-11-12T22:11:25.403332Z","shell.execute_reply":"2024-11-12T22:11:25.415452Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"array(['fear', 'angry', 'disgust', 'neutral', 'sad', 'surprised', 'happy'],\n      dtype=object)"},"metadata":{}}]},{"cell_type":"code","source":"# Combine the metadata for all dataframes!\n\ncombined_metadata_df = pd.concat([ravdess_metadata_df,\n                                  crema_organized_df,\n                                  savee_metadata_df,\n                                  tess_metadata_df])\n\ncombined_metadata_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-11-12T22:11:25.417301Z","iopub.execute_input":"2024-11-12T22:11:25.417581Z","iopub.status.idle":"2024-11-12T22:11:25.434739Z","shell.execute_reply.started":"2024-11-12T22:11:25.417551Z","shell.execute_reply":"2024-11-12T22:11:25.433837Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                   Filename  \\\n0  03-01-08-01-01-01-02.wav   \n1  03-01-01-01-01-01-02.wav   \n2  03-01-07-02-01-02-02.wav   \n3  03-01-07-01-01-02-02.wav   \n4  03-01-01-01-02-01-02.wav   \n\n                                            Filepath  Gender    Emotion  \\\n0  /kaggle/input/speech-emotion-recognition-en/Ra...  female  surprised   \n1  /kaggle/input/speech-emotion-recognition-en/Ra...  female    neutral   \n2  /kaggle/input/speech-emotion-recognition-en/Ra...  female    disgust   \n3  /kaggle/input/speech-emotion-recognition-en/Ra...  female    disgust   \n4  /kaggle/input/speech-emotion-recognition-en/Ra...  female    neutral   \n\n  Emotional Intensity  \n0              medium  \n1              medium  \n2                high  \n3              medium  \n4              medium  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Filename</th>\n      <th>Filepath</th>\n      <th>Gender</th>\n      <th>Emotion</th>\n      <th>Emotional Intensity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>03-01-08-01-01-01-02.wav</td>\n      <td>/kaggle/input/speech-emotion-recognition-en/Ra...</td>\n      <td>female</td>\n      <td>surprised</td>\n      <td>medium</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>03-01-01-01-01-01-02.wav</td>\n      <td>/kaggle/input/speech-emotion-recognition-en/Ra...</td>\n      <td>female</td>\n      <td>neutral</td>\n      <td>medium</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>03-01-07-02-01-02-02.wav</td>\n      <td>/kaggle/input/speech-emotion-recognition-en/Ra...</td>\n      <td>female</td>\n      <td>disgust</td>\n      <td>high</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>03-01-07-01-01-02-02.wav</td>\n      <td>/kaggle/input/speech-emotion-recognition-en/Ra...</td>\n      <td>female</td>\n      <td>disgust</td>\n      <td>medium</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>03-01-01-01-02-01-02.wav</td>\n      <td>/kaggle/input/speech-emotion-recognition-en/Ra...</td>\n      <td>female</td>\n      <td>neutral</td>\n      <td>medium</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"#Exploratory Data Analysis\n\nsns.countplot(data = combined_metadata_df, x = 'Emotion')\ncombined_metadata_df['Emotion'].value_counts()\n","metadata":{"execution":{"iopub.status.busy":"2024-11-12T22:11:25.436939Z","iopub.execute_input":"2024-11-12T22:11:25.437558Z","iopub.status.idle":"2024-11-12T22:11:25.769875Z","shell.execute_reply.started":"2024-11-12T22:11:25.437513Z","shell.execute_reply":"2024-11-12T22:11:25.768917Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"Emotion\ndisgust      1923\nsad          1923\nfear         1923\nhappy        1923\nangry        1923\nneutral      1895\nsurprised     652\nName: count, dtype: int64"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"markdown","source":"**Notice how surprise category is low on numbers! We should perform data augmentation only on the training dataset to avoid data leakage**\n\nThe game plan is to:\n* 1: Augment data to the surprised category to make it balanced\n* 2: Split the data into training and testing datasets\n* 3: Augment the training dataset\n","metadata":{}},{"cell_type":"code","source":"#Surprised dataframe\n\n\nsurprised_df = combined_metadata_df.loc[combined_metadata_df['Emotion'] == 'surprised']\n\nif AUGMENTATIONS_LOADED == False:\n    #Write Augmentation Functions and Helper Functions:\n    augmented_dir = \"augmented_surprised_samples\"\n    \n    #Delete everyfile in augmented_dirs_folder\n    if os.path.exists(augmented_dir):\n        shutil.rmtree(augmented_dir) \n    os.makedirs(augmented_dir, exist_ok=True)\n    \n    \n    #set random seed to be 42\n    random.seed(42)\n\ndef add_white_noise(waveform, \n                    noise_level=np.random.uniform(low =  0.0001,high = 0.001)):\n    noise = torch.randn_like(waveform, device = device) * noise_level\n    return waveform + noise\n\ndef time_stretch(waveform, sample_rate, rate=None):\n    if rate is None:\n        rate = np.random.uniform(0.8, 1.2)\n    waveform_np = waveform.squeeze().cpu().numpy()  # Convert to NumPy\n    stretched = librosa.effects.time_stretch(waveform_np, rate=rate)\n    return torch.tensor(stretched, device = device).unsqueeze(0)  # Convert back to tensor with channel dimension\n\ndef pitch_scale(waveform, sample_rate, \n                n_steps=None):\n    if n_steps is None:\n        n_steps = np.random.uniform(low = -1, high = 1)\n    waveform_np = waveform.squeeze().cpu().numpy()  # Convert to NumPy\n    pitched = librosa.effects.pitch_shift(waveform_np, sr=sample_rate, n_steps=n_steps)\n    return torch.tensor(pitched, device = device).unsqueeze(0)  # Convert back to tensor with channel dimension\n\ndef polarity_inversion(waveform):\n    return -waveform\n\ndef apply_gain(waveform, gain_factor = np.random.uniform(low = 5, high = 30)):\n    gain = torchaudio.transforms.Vol(gain = gain_factor, gain_type = 'amplitude').to(device)\n    return gain(waveform)\n\ndef apply_augmentations(waveform, sample_rate):\n    # List of possible augmentations\n    augmentations = [\n        lambda x: add_white_noise(x),\n        lambda x: time_stretch(x, sample_rate),\n        lambda x: pitch_scale(x, sample_rate),\n        lambda x: polarity_inversion(x),\n        lambda x: apply_gain(x)\n    ]\n    \n    # Randomly choose one or more augmentations to apply\n    num_augmentations = 3\n    selected_augmentations = random.sample(augmentations, num_augmentations)\n    \n    for augment in selected_augmentations:\n        waveform = augment(waveform)\n    return waveform\n\n\n# Assuming you have a dataframe 'surprised_df' with original \"surprised\" audio samples\ndef create_augmented_samples_surprised(surprised_df, new_sample_rate = None):\n    augmented_samples = []\n    \n    for version in range(1,3):\n        # First round of augmentation (AugmentedV1)\n        for i, row in tqdm(surprised_df.iterrows(), total = len(surprised_df), desc=f\"Augmenting Version {version}\"):\n            file_path = row['Filepath']\n            file_name = row['Filename'].split('.')[0]\n            \n            augmented_file_name = f\"{file_name}_surprised_augmentedV{version}_{i}.wav\"\n            gender = row['Gender']\n            emotional_intensity = row['Emotional Intensity']\n            \n            \n            waveform, original_sr = torchaudio.load(file_path)\n            waveform = waveform.to(device)\n            \n            #print(file_name)\n\n            # Ensure waveform is 2D [1, num_samples] if it's mono\n            if waveform.ndim > 1:\n                waveform = waveform.mean(dim = 0).unsqueeze(0)\n\n            #step1= time.time()\n            # Resample if necessary\n            if original_sr != new_sample_rate and new_sample_rate is not None:\n                resampler = torchaudio.transforms.Resample(original_sr, \n                                                           new_sample_rate,\n                                                           lowpass_filter_width=16, \n                                                           resampling_method='sinc_interp_hann').to(device)\n                waveform = resampler(waveform)\n\n            #step2= time.time()\n            #print(f'Resample time was {step1-step2}')\n\n            \n            # Apply augmentations to create the first round of augmented samples\n            augmented_waveform = apply_augmentations(waveform, original_sr)\n\n            augmented_waveform = augmented_waveform.cpu()\n\n            # Save augmented sample to a new file\n            augmented_file_path = os.path.join(augmented_dir, augmented_file_name)\n            torchaudio.save(augmented_file_path, augmented_waveform, original_sr, channels_first = True)\n\n            # Append augmented sample details to the list\n            augmented_samples.append({\n                'Filename': augmented_file_name,\n                'Filepath': augmented_file_path,\n                'Gender': gender,\n                'Emotion': 'surprised',  # Keep the same emotion label\n                'Emotional Intensity': emotional_intensity,\n                'Augmentation_Type': f'AugmentedV{version}'  # Tag the augmentation type\n            })\n\n    # Convert to DataFrame for compatibility\n    augmented_df = pd.DataFrame(augmented_samples)\n    return augmented_df\n\n","metadata":{"execution":{"iopub.status.busy":"2024-11-12T22:11:25.771130Z","iopub.execute_input":"2024-11-12T22:11:25.771445Z","iopub.status.idle":"2024-11-12T22:11:25.796784Z","shell.execute_reply.started":"2024-11-12T22:11:25.771411Z","shell.execute_reply":"2024-11-12T22:11:25.795663Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"#Save and run the augmentations \n#Augment only the surprised_df to create new samples:\n\nif AUGMENTATIONS_LOADED == False:\n    sample_rate = 24414 \n    augmented_surprised_df = create_augmented_samples_surprised(surprised_df, \n                                                                new_sample_rate = sample_rate) \n    \n    shutil.make_archive('augmented_surprised_samples', 'zip', augmented_dir)\n    \n    # Download the zip file (if using a Jupyter environment)\n    from IPython.display import FileLink\n    FileLink('augmented_surprised_samples.zip')","metadata":{"execution":{"iopub.status.busy":"2024-11-12T22:11:25.797891Z","iopub.execute_input":"2024-11-12T22:11:25.798261Z","iopub.status.idle":"2024-11-12T22:11:25.810339Z","shell.execute_reply.started":"2024-11-12T22:11:25.798212Z","shell.execute_reply":"2024-11-12T22:11:25.809496Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"#Combine the augmented surprised with the combined_metadata_df \n\nif AUGMENTATIONS_LOADED == False:\n    combined_metadata_df = pd.concat((combined_metadata_df,\n                                      augmented_surprised_df.drop('Augmentation_Type',axis = 1)))\n    \n    sns.countplot(data = combined_metadata_df, x = 'Emotion')\n    combined_metadata_df['Emotion'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-11-12T22:11:25.811381Z","iopub.execute_input":"2024-11-12T22:11:25.811667Z","iopub.status.idle":"2024-11-12T22:11:25.820060Z","shell.execute_reply.started":"2024-11-12T22:11:25.811638Z","shell.execute_reply":"2024-11-12T22:11:25.819195Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"#Splitting Training and Testing Data\n\n#gameplan:\n#1. Take nonaugmented surprised samples and make those the testing samples\n#2. Take the other emotions and create testing and training samples \n\n# Take nonaugmented surprised data for testing data\n\n\n#Create function for creating training and testing indices for other emotions\ndef creating_training_testing_split(combined_metadata_df, test_size ,drop_emotion = True, **kwargs):\n\n    \"\"\"\n    **kwargs can be the following:\n    emotion_to_drop = emotion\n    random_state = random_state for train_test_split\n\n    \"\"\"\n    for key,value in kwargs.items():\n        if key == 'emotion_to_drop':\n            emotion_to_drop = value\n        if key == 'random_state':\n            random_state = value\n            \n    emotions_list = list(combined_metadata_df['Emotion'].unique())\n    \n    if drop_emotion == True:\n        emotions_list.remove(emotion_to_drop)\n\n    test_indices = {}\n    train_indices = {}\n\n    training_df = pd.DataFrame(columns = combined_metadata_df.columns)\n    testing_df = pd.DataFrame(columns = combined_metadata_df.columns)\n\n    for emotion in emotions_list:\n        emotion_df = combined_metadata_df[combined_metadata_df['Emotion'] == emotion]\n        \n        #test_size = 652\n\n        train_idxs, test_idxs= train_test_split(range(len(emotion_df)), \n                                                 test_size = test_size, \n                                                 random_state = SEED)\n        #Append to dictionary  \n        test_indices[emotion] =  test_idxs\n        train_indices[emotion] =  train_idxs\n\n        emotion_df_train = emotion_df.iloc[train_indices[emotion],:]\n        emotion_df_test = emotion_df.iloc[test_indices[emotion],:]\n\n        training_df = pd.concat((training_df,emotion_df_train),axis = 0)\n        testing_df = pd.concat((testing_df,emotion_df_test),axis = 0)\n\n    return training_df, testing_df, train_indices, test_indices","metadata":{"execution":{"iopub.status.busy":"2024-11-12T22:11:25.821361Z","iopub.execute_input":"2024-11-12T22:11:25.821722Z","iopub.status.idle":"2024-11-12T22:11:25.832569Z","shell.execute_reply.started":"2024-11-12T22:11:25.821670Z","shell.execute_reply":"2024-11-12T22:11:25.831546Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"#Create the training splits by calling the function\n\nif AUGMENTATIONS_LOADED == False:\n    surprised_df = combined_metadata_df.loc[combined_metadata_df['Emotion'] == 'surprised']\n    \n    nonaugmented_surprised_df = []\n    augmented_surprised_df = []\n    \n    for i,row in surprised_df.iterrows():\n        \n        gender = row['Gender']\n        filename = row['Filename'] \n        filepath = row['Filepath']\n        emotional_int = row['Emotional Intensity']\n        emotion = row['Emotion']\n        \n        if 'surprised_augmented' not in row['Filename']:\n            \n            nonaugmented_surprised_df.append({\n                'Filename': filename,\n                'Filepath': filepath,\n                'Gender': gender,\n                'Emotion': emotion,\n                'Emotional Intensity': emotional_int\n            })\n        else:\n            augmented_surprised_df.append({\n                'Filename': filename,\n                'Filepath': filepath,\n                'Gender': gender,\n                'Emotion': emotion,\n                'Emotional Intensity': emotional_int\n            })\n            \n    nonaugmented_surprised_df = pd.DataFrame(nonaugmented_surprised_df)\n    augmented_surprised_df = pd.DataFrame(augmented_surprised_df)\n    testing_samples = nonaugmented_surprised_df.shape[0]\n    \n    \n    training_df,testing_df,_,_ = creating_training_testing_split(combined_metadata_df, \n                                    test_size = testing_samples ,\n                                    drop_emotion = True, \n                                    emotion_to_drop = 'surprised')\n    \n    #append the surprised to the training dataframes and testing dataframes\n    training_df = pd.concat((augmented_surprised_df,training_df),axis = 0)\n    testing_df = pd.concat((nonaugmented_surprised_df,testing_df),axis = 0)\n    \n    \n    training_df.shape\n    \n    sns.countplot(data = training_df, x = 'Emotion')\n    combined_metadata_df['Emotion'].value_counts()\n    plt.show()\n    \n    sns.countplot(data = testing_df, x = 'Emotion')\n    combined_metadata_df['Emotion'].value_counts()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-11-12T22:11:25.835019Z","iopub.execute_input":"2024-11-12T22:11:25.835338Z","iopub.status.idle":"2024-11-12T22:11:25.846872Z","shell.execute_reply.started":"2024-11-12T22:11:25.835289Z","shell.execute_reply":"2024-11-12T22:11:25.846157Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## Augment the training samples excluding the surprised category (since those are already augmented)","metadata":{}},{"cell_type":"code","source":"#Augment the training data\n\nif AUGMENTATIONS_LOADED == False:\n    non_surprised_df = combined_metadata_df.loc[combined_metadata_df['Emotion'] != 'surprised']\n    \n    #Write Augmentation Functions and Helper Functions:\n    augmented_dir = \"augmented_training_samples\"\n    \n    #Delete everyfile in augmented_dirs_folder\n    if os.path.exists(augmented_dir):\n        shutil.rmtree(augmented_dir) \n    os.makedirs(augmented_dir, exist_ok=True)\n\ndef create_training_augmentations(non_surprised_df, new_sample_rate):\n    augmented_training_samples = []\n    # First round of augmentation training samples (no surprised) (AugmentedV1)\n    \n    for i, row in tqdm(non_surprised_df.iterrows(),total = len(non_surprised_df),desc=f\"Augmenting Version 1\"):\n        file_path = row['Filepath']\n        file_name = row['Filename'].split('.')[0]\n\n        augmented_file_name = f\"{file_name}_augmented_training_V1_{i}.wav\"\n        gender = row['Gender']\n        emotional_intensity = row['Emotional Intensity']\n        emotion = row['Emotion']\n\n\n        waveform, original_sr = torchaudio.load(file_path)\n        waveform = waveform.to(device)\n        #print(file_name)\n\n        # Ensure waveform is 2D [1, num_samples] if it's mono\n        if waveform.ndim > 1:\n            waveform = waveform.mean(dim = 0).unsqueeze(0)\n            \n         # Resample if necessary\n        if original_sr != new_sample_rate and new_sample_rate is not None:\n            resampler = torchaudio.transforms.Resample(original_sr, \n                                                       new_sample_rate,\n                                                       lowpass_filter_width=16, \n                                                       resampling_method='sinc_interp_hann').to(device)\n            waveform = resampler(waveform)\n            \n        # Apply augmentations to create the first round of augmented samples\n        augmented_waveform = apply_augmentations(waveform, original_sr)\n        augmented_waveform = augmented_waveform.cpu()\n\n        # Save augmented sample to a new file\n        augmented_file_path = os.path.join(augmented_dir, augmented_file_name)\n        torchaudio.save(augmented_file_path, augmented_waveform, original_sr, channels_first = True)\n\n        # Append augmented sample details to the list\n        augmented_training_samples.append({\n            'Filename': augmented_file_name,\n            'Filepath': augmented_file_path,\n            'Gender': gender,\n            'Emotion': emotion,  # Keep the same emotion label\n            'Emotional Intensity': emotional_intensity,  \n            'Augmentation_Type': f'AugmentedV'  # Tag the augmentation type\n        })\n\n    # Convert to DataFrame for compatibility\n    augmented_training_df = pd.DataFrame(augmented_training_samples)\n    return augmented_training_df\n\nif AUGMENTATIONS_LOADED == False:\n    augmented_training_df = create_training_augmentations(non_surprised_df,new_sample_rate = 24414) ","metadata":{"execution":{"iopub.status.busy":"2024-11-12T22:11:26.862386Z","iopub.execute_input":"2024-11-12T22:11:26.862791Z","iopub.status.idle":"2024-11-12T22:11:26.875047Z","shell.execute_reply.started":"2024-11-12T22:11:26.862755Z","shell.execute_reply":"2024-11-12T22:11:26.874234Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"if AUGMENTATIONS_LOADED == False:\n    # Compress the augmented directory\n    shutil.make_archive('augmented_training_samples', 'zip', augmented_dir)\n    \n    # Download the zip file (if using a Jupyter environment)\n    from IPython.display import FileLink\n    FileLink('augmented_training_samples.zip')\n","metadata":{"execution":{"iopub.status.busy":"2024-11-12T22:11:27.513855Z","iopub.execute_input":"2024-11-12T22:11:27.514243Z","iopub.status.idle":"2024-11-12T22:11:27.519109Z","shell.execute_reply.started":"2024-11-12T22:11:27.514208Z","shell.execute_reply":"2024-11-12T22:11:27.518188Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"\nif AUGMENTATIONS_LOADED == False:\n    #Create the testing samples dataset for kaggle\n    testing_df.to_csv('testing_df.csv', index=False)\n    augmented_training_df.to_csv('augmented_training_df.csv', index = False)\n    print(FileLink('testing_df.csv'))\n    print(FileLink('augmented_training_df.csv'))","metadata":{"execution":{"iopub.status.busy":"2024-11-12T22:11:27.977735Z","iopub.execute_input":"2024-11-12T22:11:27.978615Z","iopub.status.idle":"2024-11-12T22:11:27.983514Z","shell.execute_reply.started":"2024-11-12T22:11:27.978574Z","shell.execute_reply":"2024-11-12T22:11:27.982521Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"#Create a folder called Metadata and Augmentations \n\nif AUGMENTATIONS_LOADED == False:\n\n    # Step 1: Create a folder for \"Metadata and Augmentations\"\n    output_folder = \"Metadata_and_Augmentations\"\n    if os.path.exists(output_folder):\n        shutil.rmtree(output_folder)  # Remove the folder if it exists to start fresh\n    os.makedirs(output_folder, exist_ok=True)\n    \n    # Step 2: Copy the necessary files to the folder\n    shutil.copy('augmented_training_df.csv', output_folder)\n    shutil.copy('testing_df.csv', output_folder)\n    \n    # Step 3: Unzip the augmented training and surprised samples folders\n    with ZipFile('augmented_training_samples.zip', 'r') as zip_ref:\n        zip_ref.extractall(os.path.join(output_folder, 'augmented_training_samples'))\n    \n    with ZipFile('augmented_surprised_samples.zip', 'r') as zip_ref:\n        zip_ref.extractall(os.path.join(output_folder, 'augmented_surprised_samples'))\n    \n    # Step 4: Create a zip file containing the \"Metadata and Augmentations\" folder\n    output_zip_file = 'Metadata_and_Augmentations.zip'\n    shutil.make_archive('Metadata_and_Augmentations', 'zip', output_folder)\n    \n    # Step 5: Provide a download link (if using a Jupyter notebook environment)\n    from IPython.display import FileLink\n    FileLink(output_zip_file)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-11-12T22:11:29.425623Z","iopub.execute_input":"2024-11-12T22:11:29.426378Z","iopub.status.idle":"2024-11-12T22:11:29.433895Z","shell.execute_reply.started":"2024-11-12T22:11:29.426339Z","shell.execute_reply":"2024-11-12T22:11:29.432947Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"os.listdir()\nfrom IPython.display import FileLink\nFileLink('Metadata_and_Augmentations.zip')","metadata":{"execution":{"iopub.status.busy":"2024-11-12T22:11:30.089016Z","iopub.execute_input":"2024-11-12T22:11:30.089955Z","iopub.status.idle":"2024-11-12T22:11:30.102892Z","shell.execute_reply.started":"2024-11-12T22:11:30.089903Z","shell.execute_reply":"2024-11-12T22:11:30.102023Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"/kaggle/input/metadata-and-augmentations/Metadata_and_Augmentations.zip","text/html":"Path (<tt>Metadata_and_Augmentations.zip</tt>) doesn't exist. It may still be in the process of being generated, or you may have the incorrect path."},"metadata":{}}]},{"cell_type":"markdown","source":"# !End of Data Preprocessing \n**(Hint Select this cell, go Run -> run before and it should run all cells before this current cell)**\n\n**If you have ran this notebook, you should get 3 new datasets**:\n* augmented-surprised-samples\n* augmented-training-samples\n* testing-samples-metadata-csv","metadata":{}},{"cell_type":"code","source":"###### from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import StandardScaler\n\n# Initialize scaler\nscaler = StandardScaler()\n\ndef extract_features(data, sample_rate):\n    # Zero Crossing Rate (ZCR)\n    zcr = np.mean(librosa.feature.zero_crossing_rate(y=data).T, axis=0)\n    \n    # Chroma STFT\n    stft = np.abs(librosa.stft(data))\n    chroma_stft = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)\n    \n    # MFCC with a fixed number of coefficients, e.g., 13\n    mfcc = np.mean(librosa.feature.mfcc(y=data, sr=sample_rate, n_mfcc=13).T, axis=0)\n    \n    # Root Mean Square (RMS)\n    rms = np.mean(librosa.feature.rms(y=data).T, axis=0)\n    \n    # Mel Spectrogram\n    mel = np.mean(librosa.feature.melspectrogram(y=data, sr=sample_rate, n_mels=128).T, axis=0)\n    \n    # Concatenate all features into one vector\n    features = np.hstack([zcr, chroma_stft, mfcc, rms, mel])\n    \n    # Normalize features\n    features = scaler.fit_transform(features.reshape(-1, 1)).flatten()\n    \n    return features\n","metadata":{"execution":{"iopub.status.busy":"2024-11-12T22:11:31.565507Z","iopub.execute_input":"2024-11-12T22:11:31.565894Z","iopub.status.idle":"2024-11-12T22:11:31.577773Z","shell.execute_reply.started":"2024-11-12T22:11:31.565858Z","shell.execute_reply":"2024-11-12T22:11:31.576660Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"\n\n#These datasets are the result of data preprocessing cells you ran before this cell. \n\n#We need to make sure the audio files are the same size when creating the dataloader:\n\n#Helper functions:\ndef get_max_audio_length(metadata_df):\n    \"\"\"Calculate the maximum length of audio samples in the dataset.\"\"\"\n    start = time.time()\n    max_length = 0\n    for file_path in metadata_df['Filepath']:\n        waveform, sample_rate = torchaudio.load(file_path)\n        num_samples = waveform.shape[1]  # Number of samples in the waveform\n        if num_samples > max_length:\n            max_length = num_samples\n    end = time.time()\n    print (f'Max_length_found: {max_length}. Took {start - end} seconds')\n    return max_length\n\ndef same_length_batch(batch):\n    # Collate function to handle variable-length sequences\n\n    # Extract waveforms, emotions, and genders from the batch\n    waveforms = [item['waveform'].squeeze(0) for item in batch]  # Remove channel dimension if present (it's all mono anyways!)\n    emotions = torch.stack([item['emotion'] for item in batch]) #example [0,1,2]\n    genders = torch.stack([item['gender'] for item in batch]) #same as above\n    intensity = torch.stack([item['intensity'] for item in batch])\n    sample_rate = torch.stack([item['sample rate'] for item in batch])\n\n    \n    # Pad all waveforms to the same length (of the longest in the batch) ****THIS MEANS WE WILL HAVE TO MAKE THE NN VARIABLE LENGTH DEPENDING ON THE BATCH!\n    waveforms_padded = pad_sequence(waveforms, batch_first=True) #put the batch dimension first!\n    \n    # Return padded waveforms and corresponding labels\n    return {'waveform': waveforms_padded, \n            'emotion': emotions, \n            'gender': genders,\n            'vocal channel': vocal_channels,\n            'intensity':intensity,\n            'statement': statement,\n            'sample rate':sample_rate}\n\n#Create the custom pytorch dataset\n\nclass Emotion_Classification_Dataset(Dataset):\n    def __init__(self, metadata_df, \n                 transform=None, \n                 same_length_all = True, \n                 target_length = None,\n                 target_sr = None,\n                 augment_surprised=False,\n                 device = device):\n        \"\"\"\n        Args:\n            metadata_df (DataFrame): DataFrame containing file paths and labels.\n            target_length (int): Target length for all audio samples in number of samples.\n            transformations (callable, optional): Optional list of transformations to be applied on a sample (e.g., audio augmentation).\n            same_length_all (Boolean): If True, enforce same length for all audio samples.\n        \"\"\"\n        self.metadata_df = metadata_df\n        self.transform = transform\n        self.same_length_all = same_length_all #Boolean\n        self.target_length = target_length\n        self.augment_surprised = augment_surprised  # Boolean for augmenting \"surprised\" category\n        self.target_sr = target_sr\n        \n    def __len__(self):\n        return len(self.metadata_df)\n\n    def __getitem__(self, idx):\n\n        start_time = time.time()\n\n    \n        # Get file path and labels from the DataFrame\n        file_path = self.metadata_df.iloc[idx]['Filepath']\n        emotion_label = self.metadata_df.iloc[idx]['Emotion']\n        gender_label = self.metadata_df.iloc[idx]['Gender']\n        intensity = self.metadata_df.iloc[idx]['Emotional Intensity']\n\n      \n        #\n        waveform, sample_rate = torchaudio.load(file_path)\n        \n  \n        waveform = waveform.to(device)\n        \n        #Resample the data\n        #waveform = self.resample_if_necessary(waveform, sample_rate) \n        \n\n        #Pad the data to make the same length\n        if self.same_length_all and self.target_length is not None:\n            waveform = self.pad_or_trim_waveform(waveform, self.target_length)\n    \n\n        #waveform.cpu()\n        if self.transform:\n            waveform = self.transform(waveform)\n                \n        waveform = self.resample_if_necessary(waveform, sample_rate)\n        \n        # Extract features (e.g., MFCCs, ZCR, etc.)\n        features = extract_features(waveform.squeeze().cpu().numpy(), sample_rate)\n        emotion_mapping = {\n            \"neutral\": 0, \"happy\": 1, \"sad\": 2, \"angry\": 3,\n            \"fear\": 4, \"disgust\": 5, \"surprised\": 6\n        }\n        gender_mapping = {\"male\": 0, \"female\": 1}\n        intensity_mapping = {\"low\": 0, \"medium\": 1, \"high\": 2, \"unknown\": 3}\n\n        emotion_tensor = torch.tensor(emotion_mapping[emotion_label])\n        gender_tensor = torch.tensor(gender_mapping[gender_label])\n        intensity_tensor = torch.tensor(intensity_mapping[intensity])\n\n        # Return the sample\n        sample = {\n    'raw_features': features,  # Original features without tensor conversion\n    'features_tensor': torch.tensor(features, dtype=torch.float32),\n    'emotion': emotion_tensor,\n    'gender': gender_tensor,\n    'intensity': intensity_tensor,\n    'sample_rate': torch.tensor(sample_rate)\n}\n        return sample\n    \n    def pad_or_trim_waveform(self, waveform, target_length):\n        \"\"\"Pad or trim waveform to a fixed target length in samples.\"\"\"\n        num_samples = waveform.shape[1]  # waveform shape is (channels, num_samples)\n\n        if num_samples < target_length:\n            # Pad if the waveform is shorter than target length\n            padding = target_length - num_samples\n            waveform = F.pad(waveform, (0, padding))#.to(device) #pad the left with 0 0s and pad the right with padding amount of 0s\n        elif num_samples > target_length:\n            # Trim if the waveform is longer than target length\n            waveform = waveform[:, :target_length]\n\n        return waveform\n    \n    # def resample_if_necessary(self, waveform, sr):\n    #     if sr != self.target_sr:\n    #         resampler = torchaudio.transforms.Resample(sr, self.target_sr)\n    #         waveform = resampler(waveform) \n            \n    #     return waveform\n\n    def resample_if_necessary(self, waveform, sr):\n        if sr != self.target_sr:\n            resampler = torchaudio.transforms.Resample(sr, self.target_sr).to(device) # Move to GPU\n            waveform = resampler(waveform)\n        return waveform\n\n\n#not in the dataloader class btw....\ndef load_dataset(metadata_df,\n                 same_length_all = True,\n                 sample_rate = 24414,\n                 seconds_of_audio = 3,\n                transformations = None):\n\n    # def worker_init_fn(worker_id):\n    #     random.seed(SEED)\n    #     np.random.seed(SEED)\n    \n    if same_length_all:\n        \n        #max length of our audio dataset (without any augmentation is 314818) \n        #max_length = get_max_audio_length(combined_metadata_df)\n        #max_length = 314818 #TRUE MAX LENGTH\n        \n        max_length = sample_rate * seconds_of_audio\n        combined_dataset = Emotion_Classification_Dataset(metadata_df=metadata_df,\n                                             same_length_all = True,\n                                                target_length = max_length,\n                                                target_sr = 24414)\n        # dataloader = DataLoader(combined_dataset, \n        #                         batch_size=16, \n        #                         shuffle=True,  \n        #                         num_workers = 1)#,persistent_workers=True)\n\n        dataloader = DataLoader(combined_dataset, \n                                batch_size=16, \n                                shuffle=True)\n        \n    else:\n        combined_dataset = Emotion_Classification_Dataset(metadata_df=metadata_df,\n                                                same_length_all = False,\n                                                target_sr = 24414)\n        dataloader = DataLoader(combined_dataset, \n                                batch_size=16, \n                                shuffle=True, \n                                collate_fn=same_length_batch, \n                                num_workers = 8,\n                                persistent_workers=True,\n                                worker_init_fn=worker_init_fn)\n    return dataloader\n","metadata":{"execution":{"iopub.status.busy":"2024-11-12T22:14:08.568430Z","iopub.execute_input":"2024-11-12T22:14:08.569081Z","iopub.status.idle":"2024-11-12T22:14:08.596341Z","shell.execute_reply.started":"2024-11-12T22:14:08.569039Z","shell.execute_reply":"2024-11-12T22:14:08.595251Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"\nstart = time.time()\n\ndataloader = load_dataset(metadata_df = augmented_training_df,\n                          same_length_all=True,\n                          seconds_of_audio = 3,\n                          ) \nend = time.time()\n\nprint(end - start)\n# Iterate over the DataLoader (Print one iteration of the batch!)\nfor batch in tqdm(dataloader):\n    features = batch['features_tensor']\n    emotions = batch['emotion']\n    genders = batch['gender']\n    intensity = batch['intensity']\n    print(features.shape,features, emotions, genders,intensity)\n    break","metadata":{"execution":{"iopub.status.busy":"2024-11-12T22:15:36.344639Z","iopub.execute_input":"2024-11-12T22:15:36.345035Z","iopub.status.idle":"2024-11-12T22:16:22.913913Z","shell.execute_reply.started":"2024-11-12T22:15:36.344998Z","shell.execute_reply":"2024-11-12T22:16:22.912679Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"0.00037789344787597656\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/720 [00:46<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"torch.Size([16, 155]) tensor([[ 0.0467,  0.0599,  0.0606,  ...,  0.0459,  0.0459,  0.0459],\n        [ 0.0810,  0.0857,  0.0856,  ...,  0.0786,  0.0786,  0.0786],\n        [-0.4191, -0.4150, -0.4152,  ..., -0.3869, -0.4053, -0.4189],\n        ...,\n        [ 0.0864,  0.0957,  0.0960,  ...,  0.0803,  0.0803,  0.0803],\n        [ 0.0580,  0.0669,  0.0674,  ...,  0.0574,  0.0574,  0.0574],\n        [-0.2534, -0.2377, -0.2367,  ..., -0.2543, -0.2543, -0.2543]]) tensor([4, 1, 1, 5, 1, 2, 3, 1, 4, 5, 2, 1, 3, 5, 5, 4]) tensor([0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1]) tensor([0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 1])\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"#Play audio file\n#Audio(batch['waveform_features'][5].squeeze(), rate = 24414)\n\nbatch['waveform_features']['original waveform'].shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Model Building","metadata":{}},{"cell_type":"code","source":"#mode (CNN)","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nmax_length = get_max_audio_length(augmented_training_df)\n\nclass SER_CNN(nn.Module):\n    def __init__(self, num_classes=7):\n        super(SER_CNN, self).__init__()\n        \n        self.conv1 = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=2)\n        self.bn1 = nn.BatchNorm1d(16)\n        self.pool = nn.MaxPool1d(2)\n        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=2)\n        self.bn2 = nn.BatchNorm1d(32)\n        self.fc1 = nn.Linear(1216, 128) \n        self.fc2 = nn.Linear(128, num_classes)\n        self.relu = nn.ReLU()\n        \n    def forward(self, x):\n        x = self.relu(self.bn1(self.conv1(x)))\n        x = self.pool(x)\n        x = self.relu(self.bn2(self.conv2(x)))\n        x = self.pool(x)\n        x = x.view(x.size(0), -1)\n        print(\"Shape before fc1:\", x.shape)# Flatten\n        x = self.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n# Initialize model, loss function, and optimizer\nnum_classes = 7  # Number of emotion classes\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = SER_CNN(num_classes=num_classes).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-12T22:17:18.260677Z","iopub.execute_input":"2024-11-12T22:17:18.261680Z","iopub.status.idle":"2024-11-12T22:21:28.179007Z","shell.execute_reply.started":"2024-11-12T22:17:18.261636Z","shell.execute_reply":"2024-11-12T22:21:28.177997Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Max_length_found: 181339. Took -248.93470239639282 seconds\n","output_type":"stream"}]},{"cell_type":"code","source":"def train(model, dataloader, criterion, optimizer):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    for batch in dataloader:\n        # Transfer batch data to the GPU\n        features = batch['features_tensor'].to(device)\n        emotions = batch['emotion'].to(device)\n        # Add channel dimension for Conv1D\n        features = features.unsqueeze(1)\n\n        # Forward pass\n        outputs = model(features)\n        loss = criterion(outputs, emotions)\n        \n        # Backward pass and optimization\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        # Track accuracy and loss\n        _, predicted = torch.max(outputs, 1)\n        total += emotions.size(0)\n        correct += (predicted == emotions).sum().item()\n        running_loss += loss.item()\n\n    epoch_loss = running_loss / len(dataloader)\n    epoch_acc = 100 * correct / total\n    return epoch_loss, epoch_acc\ndef evaluate(model, dataloader, criterion):\n    model.eval()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    with torch.no_grad():\n        for batch in dataloader:\n            # Transfer batch data to the GPU\n            features = batch['features_tensor'].to(device)\n            emotions = batch['emotion'].to(device)\n\n            # Add channel dimension for Conv1D\n            features = features.unsqueeze(1)\n\n            # Forward pass\n            outputs = model(features)\n            loss = criterion(outputs, emotions)\n\n            # Track accuracy and loss\n            _, predicted = torch.max(outputs, 1)\n            total += emotions.size(0)\n            correct += (predicted == emotions).sum().item()\n            running_loss += loss.item()\n\n    epoch_loss = running_loss / len(dataloader)\n    epoch_acc = 100 * correct / total\n    return epoch_loss, epoch_acc\n","metadata":{"execution":{"iopub.status.busy":"2024-11-12T22:23:49.498184Z","iopub.execute_input":"2024-11-12T22:23:49.499281Z","iopub.status.idle":"2024-11-12T22:23:49.514318Z","shell.execute_reply.started":"2024-11-12T22:23:49.499226Z","shell.execute_reply":"2024-11-12T22:23:49.513233Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"train_loader = load_dataset(metadata_df = augmented_training_df,\n                          same_length_all=True,\n                          seconds_of_audio = 3,\n                          ) \ntest_loader = load_dataset(metadata_df = testing_df,\n                          same_length_all=True,\n                          seconds_of_audio = 3,\n                          ) \n\n# Training configuration\n","metadata":{"execution":{"iopub.status.busy":"2024-11-12T22:23:50.894491Z","iopub.execute_input":"2024-11-12T22:23:50.895419Z","iopub.status.idle":"2024-11-12T22:23:50.900348Z","shell.execute_reply.started":"2024-11-12T22:23:50.895377Z","shell.execute_reply":"2024-11-12T22:23:50.899262Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"num_epochs = 20\n\nfor epoch in range(num_epochs):\n    train_loss, train_acc = train(model, train_loader, criterion, optimizer)\n    val_loss, val_acc = evaluate(model, test_loader, criterion)\n    \n    print(f'Epoch [{epoch+1}/{num_epochs}]')\n    print(f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.2f}%')\n    print(f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2024-11-12T22:23:51.742427Z","iopub.execute_input":"2024-11-12T22:23:51.743131Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Shape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\nShape before fc1: torch.Size([16, 1216])\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}