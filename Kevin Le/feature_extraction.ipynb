{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kevin.KEVIN\\anaconda3\\Lib\\site-packages\\paramiko\\pkey.py:82: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"cipher\": algorithms.TripleDES,\n",
      "c:\\Users\\kevin.KEVIN\\anaconda3\\Lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.Blowfish and will be removed from this module in 45.0.0.\n",
      "  \"class\": algorithms.Blowfish,\n",
      "c:\\Users\\kevin.KEVIN\\anaconda3\\Lib\\site-packages\\paramiko\\transport.py:243: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"class\": algorithms.TripleDES,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\kevin.KEVIN\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from skimage.transform import resize\n",
    "\n",
    "from tensorflow.keras import layers, models, optimizers, regularizers\n",
    "from tensorflow.keras import Model\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC, Wav2Vec2Config\n",
    "import torchaudio\n",
    "from torchaudio.transforms import Resample\n",
    "import torch\n",
    "import tensorflow_hub as hub\n",
    "import kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Filename</th>\n",
       "      <th>Filepath</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Emotional Intensity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>03-01-01-01-01-01-01.wav</td>\n",
       "      <td>Data/speech-emotion-recognition-en/Ravdess/aud...</td>\n",
       "      <td>male</td>\n",
       "      <td>neutral</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>03-01-01-01-01-02-01.wav</td>\n",
       "      <td>Data/speech-emotion-recognition-en/Ravdess/aud...</td>\n",
       "      <td>male</td>\n",
       "      <td>neutral</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>03-01-01-01-02-01-01.wav</td>\n",
       "      <td>Data/speech-emotion-recognition-en/Ravdess/aud...</td>\n",
       "      <td>male</td>\n",
       "      <td>neutral</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>03-01-01-01-02-02-01.wav</td>\n",
       "      <td>Data/speech-emotion-recognition-en/Ravdess/aud...</td>\n",
       "      <td>male</td>\n",
       "      <td>neutral</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>03-01-02-01-01-01-01.wav</td>\n",
       "      <td>Data/speech-emotion-recognition-en/Ravdess/aud...</td>\n",
       "      <td>male</td>\n",
       "      <td>neutral</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                  Filename  \\\n",
       "0           0  03-01-01-01-01-01-01.wav   \n",
       "1           1  03-01-01-01-01-02-01.wav   \n",
       "2           2  03-01-01-01-02-01-01.wav   \n",
       "3           3  03-01-01-01-02-02-01.wav   \n",
       "4           4  03-01-02-01-01-01-01.wav   \n",
       "\n",
       "                                            Filepath Gender  Emotion  \\\n",
       "0  Data/speech-emotion-recognition-en/Ravdess/aud...   male  neutral   \n",
       "1  Data/speech-emotion-recognition-en/Ravdess/aud...   male  neutral   \n",
       "2  Data/speech-emotion-recognition-en/Ravdess/aud...   male  neutral   \n",
       "3  Data/speech-emotion-recognition-en/Ravdess/aud...   male  neutral   \n",
       "4  Data/speech-emotion-recognition-en/Ravdess/aud...   male  neutral   \n",
       "\n",
       "  Emotional Intensity  \n",
       "0              medium  \n",
       "1              medium  \n",
       "2              medium  \n",
       "3              medium  \n",
       "4              medium  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"combined_metadata_df.csv\")\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Emotion\n",
       "happy      1923\n",
       "sad        1923\n",
       "angry      1923\n",
       "fear       1923\n",
       "disgust    1923\n",
       "neutral    1895\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = train_data[~(train_data['Emotion']=='surprised')]\n",
    "train_data['Emotion'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Filename</th>\n",
       "      <th>Filepath</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Emotional Intensity</th>\n",
       "      <th>emotion_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>03-01-01-01-01-01-01.wav</td>\n",
       "      <td>Data/speech-emotion-recognition-en/Ravdess/aud...</td>\n",
       "      <td>male</td>\n",
       "      <td>neutral</td>\n",
       "      <td>medium</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>03-01-01-01-01-02-01.wav</td>\n",
       "      <td>Data/speech-emotion-recognition-en/Ravdess/aud...</td>\n",
       "      <td>male</td>\n",
       "      <td>neutral</td>\n",
       "      <td>medium</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>03-01-01-01-02-01-01.wav</td>\n",
       "      <td>Data/speech-emotion-recognition-en/Ravdess/aud...</td>\n",
       "      <td>male</td>\n",
       "      <td>neutral</td>\n",
       "      <td>medium</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>03-01-01-01-02-02-01.wav</td>\n",
       "      <td>Data/speech-emotion-recognition-en/Ravdess/aud...</td>\n",
       "      <td>male</td>\n",
       "      <td>neutral</td>\n",
       "      <td>medium</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>03-01-02-01-01-01-01.wav</td>\n",
       "      <td>Data/speech-emotion-recognition-en/Ravdess/aud...</td>\n",
       "      <td>male</td>\n",
       "      <td>neutral</td>\n",
       "      <td>medium</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                  Filename  \\\n",
       "0           0  03-01-01-01-01-01-01.wav   \n",
       "1           1  03-01-01-01-01-02-01.wav   \n",
       "2           2  03-01-01-01-02-01-01.wav   \n",
       "3           3  03-01-01-01-02-02-01.wav   \n",
       "4           4  03-01-02-01-01-01-01.wav   \n",
       "\n",
       "                                            Filepath Gender  Emotion  \\\n",
       "0  Data/speech-emotion-recognition-en/Ravdess/aud...   male  neutral   \n",
       "1  Data/speech-emotion-recognition-en/Ravdess/aud...   male  neutral   \n",
       "2  Data/speech-emotion-recognition-en/Ravdess/aud...   male  neutral   \n",
       "3  Data/speech-emotion-recognition-en/Ravdess/aud...   male  neutral   \n",
       "4  Data/speech-emotion-recognition-en/Ravdess/aud...   male  neutral   \n",
       "\n",
       "  Emotional Intensity  emotion_label  \n",
       "0              medium              4  \n",
       "1              medium              4  \n",
       "2              medium              4  \n",
       "3              medium              4  \n",
       "4              medium              4  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_labels = train_data['Emotion'].unique().tolist()\n",
    "emotion_keys = set(train_data.Emotion)\n",
    "emotion_values = np.arange(0, len(emotion_keys))\n",
    "emotion_dict = dict(zip(sorted(emotion_keys), emotion_values))\n",
    "emotion_class=len(emotion_labels)\n",
    "\n",
    "train_data['emotion_label'] = train_data['Emotion'].apply(lambda x: emotion_dict[x])\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\kevin.KEVIN\\anaconda3\\Lib\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\kevin.KEVIN\\anaconda3\\Lib\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\kevin.KEVIN\\anaconda3\\Lib\\site-packages\\tensorflow_hub\\module_v2.py:126: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\kevin.KEVIN\\anaconda3\\Lib\\site-packages\\tensorflow_hub\\module_v2.py:126: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "yamnet_model = hub.load('https://www.kaggle.com/models/google/yamnet/TensorFlow2/yamnet/1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def audio_processing(file_path, target_duration=3, target_sample_rate=16000, device=None, target_embedding_dim = 256):\n",
    "    # Set the device (CPU or CUDA)\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load the audio file\n",
    "    waveform, sample_rate = torchaudio.load(file_path)\n",
    "    waveform = waveform.to(device)  # Move waveform to the device\n",
    "\n",
    "    # Convert to mono if stereo\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "\n",
    "    # Resample if the sample rate is different from the target\n",
    "    if sample_rate != target_sample_rate:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=target_sample_rate).to(device)\n",
    "        waveform = resampler(waveform)\n",
    "        sample_rate = target_sample_rate\n",
    "\n",
    "    # Ensure the audio is of target duration\n",
    "    target_length = target_sample_rate * target_duration\n",
    "    num_samples = waveform.shape[1]\n",
    "\n",
    "    if num_samples < target_length:\n",
    "        # Pad with zeros at the end\n",
    "        padding = target_length - num_samples\n",
    "        waveform = torch.nn.functional.pad(waveform, (0, padding), mode=\"constant\", value=0)\n",
    "    elif num_samples > target_length:\n",
    "        # Truncate to the target length\n",
    "        waveform = waveform[:, :target_length]\n",
    "\n",
    "    # Process with YAMNet model (requires NumPy for YAMNet)\n",
    "    waveform_np = waveform.squeeze().cpu().numpy()  # Convert to NumPy for compatibility with YAMNet\n",
    "    scores, embeddings, log_mel_spectrogram = yamnet_model(waveform_np)\n",
    "\n",
    "    # Check output shapes\n",
    "    assert scores.shape[1] == 521, \"Scores output shape mismatch.\"\n",
    "    assert embeddings.shape[1] == 1024, \"Embeddings output shape mismatch.\"\n",
    "    assert log_mel_spectrogram.shape[1] == 64, \"Log Mel Spectrogram shape mismatch.\"\n",
    "\n",
    "    return scores, embeddings, log_mel_spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing audio files: 100%|██████████| 11510/11510 [41:33<00:00,  4.62it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize lists\n",
    "X_train_embeddings = []\n",
    "X_train_spectrograms = []\n",
    "y_train = [] \n",
    "labels = []\n",
    "\n",
    "# Define the number of emotion classes\n",
    "emotion_class = 6  # Replace with your actual number of classes\n",
    "\n",
    "# Iterate over the training data\n",
    "for index, file in tqdm(enumerate(train_data['Filepath']), total=len(train_data['Filepath']), desc=\"Processing audio files\"):\n",
    "    _, embedding, log_mel_spectrogram = audio_processing(file)  # Ensure this function is defined\n",
    "    X_train_embeddings.append(embedding)\n",
    "    X_train_spectrograms.append(log_mel_spectrogram)\n",
    "    \n",
    "    # Get the label using integer-based indexing\n",
    "    label = train_data['emotion_label'].iloc[index]\n",
    "    \n",
    "    # Ensure the label is an integer\n",
    "    label = int(label)\n",
    "    \n",
    "    # Create one-hot encoded label\n",
    "    temp = np.zeros(emotion_class)\n",
    "    temp[int(label)] = 1\n",
    "    y_train.append(temp)\n",
    "    \n",
    "    labels.append(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"X_train_embeddings.npy\", X_train_embeddings)\n",
    "np.save(\"X_train_spectrograms.npy\", X_train_spectrograms)\n",
    "np.save(\"y_train.npy\", y_train)\n",
    "np.save(\"labels.npy\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
